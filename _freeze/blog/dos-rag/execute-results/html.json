{
  "hash": "4198ac48e21ff056506a96f4a4698b12",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Implementing RAG with Document Original Structure\"\ndate: \"2025-06-26\"\ncategories: [ai, llm, rag, notebook]\nsocial-share: true\n---\n\n## 1. Background on RAG\n\nRetrieval augmented generation is one of the most popular applications of LLMs. It involves feeding the LLM with context that informs its generation, thus grounding the response in our custom data. Why would we want to do that? As good as LLMs are at language understanding, their knowledge is still frozen in time, in terms of their knowledge cutoff. So, when we need to supply the LLM with some external context, we use RAG.\n\nGiven a particular query, RAG works around the limited input token window size that an LLM has by only supplying relevant context.\n\nAt a high level, a typical RAG workflow looks like the following:\n\n![RAG Workflow](../img/rag-workflow.png){#fig-rag-workflow}\n\nIn this notebook, we will implement a type of RAG pipeline which proves to be a very strong baseline among many known RAG methods, as observed in this [research paper](http://arxiv.org/abs/2506.03989). It's called Document Original Structure RAG (DOS-RAG). The core idea is that after similarity matching, the retrieved document chunks are ordered based on their original order in the document rather than sorting by chunk score.\n\nLet's get started!\n\n## 2. Load the data\n\nWe will be using a chapter from a science textbook to demonstrate this technique. Let's see what it looks like!\n\n::: {#fbcf8a1e .cell execution_count=1}\n``` {.python .cell-code}\nimport pymupdf  # PyMuPDF\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport io\n\n# Read the PDF file\npdf_path = \"../assets/dos-rag/atoms and molecules.pdf\"\ndoc = pymupdf.open(pdf_path)\n\n# Get the first page\nfirst_page = doc[0]\n\n# Convert page to image\nmat = first_page.get_pixmap(matrix=pymupdf.Matrix(2, 2))  # 2x zoom for better quality\nimg_data = mat.tobytes(\"png\")\n# Convert to PIL Image for display\nimg = Image.open(io.BytesIO(img_data))\n# Get image dimensions\nwidth, height = img.size\n\n# Crop the top half\ntop_half = img.crop((0, 0, width, height // 2))\n\n# Display the top half\nplt.figure(figsize=(10, 6))\n# Display the image\nplt.figure(figsize=(10, 12))\nplt.imshow(top_half)\nplt.axis(\"off\")\nplt.show()\n# Close the document\ndoc.close()\n```\n\n::: {.cell-output .cell-output-display}\n```\n<Figure size 960x576 with 0 Axes>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](dos-rag_files/figure-html/cell-2-output-2.png){}\n:::\n:::\n\n\n---\n\nWe will be using [lamaindex](https://docs.llamaindex.ai/en/stable/) which is a popular framework to do all things related to\nRAG and more. We will first load the pdf into a datastructure called [`document`](https://docs.llamaindex.ai/en/stable/) and then split the given documents in [chunks](https://stackoverflow.blog/2024/12/27/breaking-up-is-hard-to-do-chunking-in-rag-applications/).We'll use the `SimpleDirectoryReader` and `TextSplitter` utilities provided by LlamaIndex.\n\n::: {#29771a25 .cell execution_count=2}\n``` {.python .cell-code}\nfrom llama_index.core import SimpleDirectoryReader\nfrom llama_index.core.node_parser import SentenceSplitter\n\n# Load the PDF as a document\ndocuments = SimpleDirectoryReader(input_files=[pdf_path]).load_data()\n\n# Let's inspect the first document\nprint(f\"Number of documents loaded: {len(documents)}\")\nprint(\"Preview of the first document:\")\nprint(documents[0].text[:500])  # Show the first 500 characters\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of documents loaded: 12\nPreview of the first document:\nAncient Indian and Greek philosophers have\nalways wonder ed about the unknown and\nunseen form of matter. The idea of divisibility\nof matter was considered long back in India,\naround 500 BC. An Indian philosopher\nMaharishi Kanad, postulated that if we go on\ndividing matter (padarth), we shall get smaller\nand smaller particles. Ultimately, a stage will\ncome when we shall come across the smallest\nparticles beyond which further division will\nnot be possible.  He named these particles\nPar manu . Anot\n```\n:::\n:::\n\n\n---\n\nNow, let's split the document into chunks. Chunking is important for efficient retrieval and to fit within the LLM's context window. We are usingt the `SentenceSplitter` class that creates chunks (nodes) keeping in mind proper sentence boundary. Also note that the parameters of `chunk_size` and `chunk_overlap` are set this way for this demo.\n\n::: {#98636cb7 .cell execution_count=3}\n``` {.python .cell-code}\n# Split the document into nodes\nsplitter = SentenceSplitter(chunk_size=512, chunk_overlap=128)\nnodes = splitter.get_nodes_from_documents(documents)\n\nprint(f\"Number of nodes created: {len(nodes)}\")\nprint(\"Preview of the first node:\")\nprint(nodes[0].text)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of nodes created: 22\nPreview of the first node:\nAncient Indian and Greek philosophers have\nalways wonder ed about the unknown and\nunseen form of matter. The idea of divisibility\nof matter was considered long back in India,\naround 500 BC. An Indian philosopher\nMaharishi Kanad, postulated that if we go on\ndividing matter (padarth), we shall get smaller\nand smaller particles. Ultimately, a stage will\ncome when we shall come across the smallest\nparticles beyond which further division will\nnot be possible.  He named these particles\nPar manu . Another Indian philosopher ,\nPakudha Katyayama, elaborated this doctrine\nand said that these particles nor mally exist\nin a combined for m which gives us various\nforms of matter.\nAround the same era, ancient Gr eek\nphilosophers – Democritus and Leucippus\nsuggested that if we go on dividing matter, a\nstage will come when particles obtained\ncannot be divided further. Democritus called\nthese indivisible particles atoms (meaning\nindivisible). All this was based on\nphilosophical considerations and not much\nexperimental work to validate these ideas\ncould be done till the eighteenth century.\nBy the end of the eighteenth century,\nscientists r\necognised the difference between\nelements and compounds and naturally\nbecame interested in finding out how and why\nelements combine and what happens when\nthey combine.\nAntoine L. Lavoisier laid the foundation\nof chemical sciences by establishing two\nimportant laws of chemical combination.\n3.1 Laws of Chemical Combination\nThe following two laws of chemical\ncombination were established after\nmuch experimentations by Lavoisier and\nJoseph L. Proust.\n3.1.1 LAW OF CONSERVATION OF MASS\nIs there a change in mass when a chemical\nchange (chemical reaction) takes place?\nActivity ______________ 3.1\n• Take one of the following sets, X and Y\nof chemicals—\nX Y\n(i) copper sulphate sodium carbonate\n(ii) barium chloride sodium sulphate\n(iii) lead nitrate sodium chloride\n• Prepare separately a 5% solution of\nany one pair of substances listed\nunder X and Y each in 10 mL in water.\n```\n:::\n:::\n\n\n---\n\n\nFor DOS-RAG to work we need the order information from the document e.g. page number and reading order. Let's access `metadata` of `node` to observe what we have. \n\n::: {#b9c80173 .cell execution_count=4}\n``` {.python .cell-code}\nprint(\"Node info: \", nodes[0].get_node_info())\nprint(\"Metadata: \", nodes[0].get_metadata_str())\n# Let's add the start idx of the node to the metadata, it will be used to order the nodes\nfor node in nodes:\n    node.metadata[\"start_idx\"] = node.get_node_info()[\"start\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNode info:  {'start': 0, 'end': 2011}\nMetadata:  page_label: 1\nfile_name: atoms and molecules.pdf\nfile_path: ../assets/dos-rag/atoms and molecules.pdf\nfile_type: application/pdf\nfile_size: 1410789\ncreation_date: 2025-07-09\nlast_modified_date: 2025-07-09\n```\n:::\n:::\n\n\nWe have the page number and reading order information in the metadata. We will use this information to order the chunks post similarity matching and retrieval.\n\n## 3. Similarity matching and retrieval\n\nWe will be using the `VectorStoreIndex` class to create an index of the nodes. We will use the `SimpleDirectoryReader` to load the data and the `SentenceSplitter` to split the document into chunks. We are using the `Gemini` model from google to create embeddings.\n\n\n\n::: {#4c3a10ae .cell execution_count=6}\n``` {.python .cell-code}\nimport os\nfrom llama_index.core import VectorStoreIndex\nfrom llama_index.core.ingestion import IngestionPipeline\nfrom llama_index.embeddings.gemini import GeminiEmbedding\nfrom llama_index.core import Settings\n\nmodel_name = \"models/embedding-001\"\n\nembed_model = GeminiEmbedding(model_name=model_name)\n\nindex = VectorStoreIndex(nodes, embed_model=embed_model)\n\nquery_engine = index.as_retriever(similarity_top_k=5)\n\nretrieved_nodes = query_engine.retrieve(\"What are atoms ?\")\n\nfor rn in retrieved_nodes:\n    node = rn.node\n    print(\"Page number: \", node.metadata[\"page_label\"])\n    print(\"Score: \", rn.score)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPage number:  5\nScore:  0.7301485048759835\nPage number:  3\nScore:  0.7032064532104829\nPage number:  5\nScore:  0.6937391396719613\nPage number:  3\nScore:  0.6851496799921348\nPage number:  6\nScore:  0.6698174569203865\n```\n:::\n:::\n\n\n---\n\nNow, let's order the retrieved nodes based on the page number and reading order as part of post processing.\n\n::: {#1f52e5f1 .cell execution_count=7}\n``` {.python .cell-code}\nfrom llama_index.core import QueryBundle\nfrom llama_index.core.postprocessor.types import BaseNodePostprocessor\nfrom llama_index.core.schema import NodeWithScore\nfrom llama_index.llms.google_genai import GoogleGenAI\nfrom llama_index.core import Settings\nfrom functools import cmp_to_key\nfrom typing import Optional\n\n\nclass DOSRAGNodePostprocessor(BaseNodePostprocessor):\n    def _postprocess_nodes(\n        self, nodes: list[NodeWithScore], query_bundle: Optional[QueryBundle]\n    ) -> list[NodeWithScore]:\n        \"\"\"\n        This postprocessor orders the retrieved nodes based on the page number and reading order.\n        \"\"\"\n\n        nodes = sorted(nodes, key=cmp_to_key(self._compare_nodes))\n\n        return nodes\n\n    def _compare_nodes(self, a: NodeWithScore, b: NodeWithScore) -> int:\n        \"\"\"\n        Compare two nodes based on the page number and reading order.\n        \"\"\"\n        if a.node.metadata[\"page_label\"] == b.node.metadata[\"page_label\"]:\n            return -1 if a.node.metadata[\"start_idx\"] < b.node.metadata[\"start_idx\"] else 1\n        else:\n            return -1 if a.node.metadata[\"page_label\"] < b.node.metadata[\"page_label\"] else 1\n\n\nSettings.llm = GoogleGenAI(model=\"gemini-2.5-flash-lite-preview-06-17\")\n\nquery_engine_0 = index.as_query_engine(node_postprocessors=[DOSRAGNodePostprocessor()], similarity_top_k=5)\n\nretrieved_nodes = query_engine_0.retrieve(\"What are atoms ?\")\n\nfor rn in retrieved_nodes:\n    node = rn.node\n    print(\"Page number: \", node.metadata[\"page_label\"])\n    print(\"Score: \", rn.score)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPage number:  3\nScore:  0.7032064532104829\nPage number:  3\nScore:  0.6851496799921348\nPage number:  5\nScore:  0.6937391396719613\nPage number:  5\nScore:  0.7301485048759835\nPage number:  6\nScore:  0.6698174569203865\n```\n:::\n:::\n\n\n---\n\nAs we can see, post processing the nodes sorts them based on the page number and reading order rather than the score, which is the core idea behind DOS-RAG. This simple post processing step is enough to setup a good baseline for a RAG application.\n\n## Summary\n\n1. RAG is a powerful technique to improve the accuracy of LLM responses by providing it with relevant context.\n2. DOS-RAG is a simple post processing step that can be used to order the retrieved nodes based on the page number and reading order.\n3. We take a document and split it into chunks, create an index of the chunks and then retrieve the most relevant chunks based on the query.\n4. Using simple sorting of the retrieved nodes based on the page number and reading order, we can achieve a good baseline for a RAG application.\n\n## References\n\n- [Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models](https://arxiv.org/abs/2506.03989)\n- [LlamaIndex](https://docs.llamaindex.ai/en/stable/)\n\n",
    "supporting": [
      "dos-rag_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}